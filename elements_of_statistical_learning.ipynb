{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读书笔记：统计学习基础（第2版）（英文）\n",
    "### The Elements of Statistical Learning: Data Mining, Inference, and Prediction\n",
    "\n",
    "### 笔记说明\n",
    "本读书笔记的作用在于：\n",
    "* 推导书中的一些数学结论；\n",
    "* 梳理本人感兴趣的主要内容，以做记录。\n",
    "\n",
    "如果想要对机器学习的监督学习做系统地了解，本人建议直接阅读原书，他人的读书笔记只能作为参考。\n",
    "\n",
    "若本读书笔记的某些公式无法在github网页上正确显示，请下载后在本地打开阅读。\n",
    "\n",
    "### 原书链接\n",
    "\n",
    "* [纸质书购买](https://www.amazon.cn/dp/B00PRH2BXA/ref=sr_1_2?ie=UTF8&qid=1547965512&sr=8-2&keywords=%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0)\n",
    "* [官方免费电子版下载](https://web.stanford.edu/~hastie/ElemStatLearn/)\n",
    "\n",
    "### 更新记录\n",
    "* 01/22/2019 - 完成第二章：监督学习综述\n",
    "* 02/15/2019 - 完成第三章：线性回归方法\n",
    "* 02/16/2019 - 完成第四、七、八和九章：线性分类方法、模型评估和选择、模型推断和平均和可加模型，树模型以及相关方法\n",
    "* 02/17/2019 - 完成第十、十一、十五章：Boosting与可加树模型、神经网络和随机森林\n",
    "\n",
    "### 主要内容\n",
    "#### 第一章：介绍（Introduction），略\n",
    "#### 第二章：[监督学习综述（Overview of Supervised Learning）](#overview)\n",
    "#### 第三章：[线性回归方法（Linear Methods for Regression）](#lr)\n",
    "#### 第四章：[线性分类方法（Linear Methods for Classification）](#lc)\n",
    "#### 第五章：基扩展与正则化，略\n",
    "#### 第六章：核平滑方法，略\n",
    "#### 第七章：[模型评估和选择（Model Assessment and Selection）](#assessment)\n",
    "#### 第八章：[模型推断和平均（Model Inference and Averaging）](#inference)\n",
    "#### 第九章：[可加模型，树模型以及相关方法（Additive Models, Trees, and Related Methods）](#tree)\n",
    "#### 第十章：[Boosting与可加树模型（Boosting and Additive Trees）](#boosting)\n",
    "#### 第十一章：[神经网络（Neural Networks）](#nn)\n",
    "#### 第十二章：支持向量机与灵活判别（Support Vector Machines and Flexible Discriminants），待定\n",
    "#### 第十三章：原型方法与最邻近算法（Prototype Methods and Nearest-Neighbors），待定\n",
    "#### 第十四章：非监督学习（Unsupervised Learning），待定\n",
    "#### 第十五章：[随机森林（Random Forest）](#rf)\n",
    "#### 第十六章：集成方法（Ensemble Learning），待定\n",
    "#### 第十七章：无向图模型（Undirected Graphical Models），待定\n",
    "#### 第十八章：高维度问题：$p\\gg N$（High-Dimensional Problems: $p\\gg N$），待定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='overview'></a>2 监督学习综述（Overview of Supervised Learning）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 统计决策理论（Statistical Decision Theory）\n",
    "监督学习的数学理论是统计决策理论，对于统计决策理论的介绍也是本书区别于其他机器学习教材和课程的特色之一。只有理解了统计决策理论，才能把各种监督学习算法联系起来，并以此为基础创造出新的变化。\n",
    "\n",
    "统计决策理论讨论的问题为：输入变量$X$和输出变量$Y$均为随机变量。求函数$f(X)$，对指定的$X=x$，$f(x)$可以对$Y$做出预测。\n",
    "\n",
    "利用统计决策理论求得$f(X)$的过程为：\n",
    "\n",
    "1. 定义损失函数（loss function）$L(Y,f(X))$。损失函数描述预测值$f(X)$与$Y$之间的差异。\n",
    "2. 最小化损失函数的期望$E(L(Y,f(X)))$，得到对应的$f(X)$。\n",
    "\n",
    "不同的损失函数所对应的$f(x)$有所不同。若定义损失函数为平方损失：$L(Y,f(X))=(Y-f(X))^2$，此时求得$f(x)$为：\n",
    "\n",
    "$$f(x)=E(Y│X=x)$$\n",
    "\n",
    "即对任意$X=x$，$f(x)$所做的预测为当$X=x$时$Y$的条件期望。下面对上述结论进行证明。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 证明：平方损失对应的解为回归函数$f(x)=E(Y│X=x)$。\n",
    "\n",
    "假设$Z=(Y-f(X))^2$，其中$X$和$Y$均为随机变量。此问题的目标是求得函数$f(X)$，使得$E(Z)$最小。\n",
    "\n",
    "由连续型随机变量的数学期望定义可得：\n",
    "$$E(Z)=\\int(y-f(x))^2g(x,y)dxdy$$\n",
    "其中$g(x,y)$为$(X,Y)$的联合概率密度。\n",
    "\n",
    "又连续型随机变量的条件分布为：$g_{Y|X}=g(x,y)/g_x(x)$，所以：\n",
    "$$E(Z)=\\int(y-f(x))^2g_{Y|X}g_X(x)dxdy=\\int_xg_X(x)dx\\int_y(y-f(x))^2g_{Y|X}dy$$\n",
    "\n",
    "所以要使得$E(Z)$最小，只需要$\\int_y(y-f(x))^2g_{Y|X}dy$最小即可。\n",
    "\n",
    "下面利用随机变量的方差的性质：对一切实数$C$，$DX=E(X-EX)^2\\leqslant E(X-C)^2$。\n",
    "\n",
    "所以：\n",
    "$$\\int_y(y-f(x))^2g_{Y|X}dy=E[(Y-f(X))^2|X=x]\\geqslant E[(Y-E(Y|X=x)^2|X=x]$$\n",
    "\n",
    "即当$f(x)=E(Y|X=x)$时，$E(Z)$取得最小值，得证。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面对统计决策理论的讨论都是基于回归问题。分类问题与回归问题类似，下面进行说明。\n",
    "\n",
    "对于任意损失函数：$L(G,\\hat G(X))$，由统计决策理论求得的$\\hat G(x)$（类比于分类问题中的$f(x)$）为：\n",
    "$$\\hat G(x)=argmin_{g\\in G}\\sum_{k=1}^KL(G_k,g)Pr⁡(G_k|X=x)$$\n",
    "\n",
    "若定义0-1损失：$L(G,\\hat G(X))=1\\cdot I(G\\neq \\hat G(X))$，那么所求得的$\\hat G(x)$为：\n",
    "\n",
    "$$\\hat G(x)=argmin_{g\\in G}[1-Pr⁡(g│X=x)]$$\n",
    "\n",
    "即对任意$X=x$，$\\hat G(X)$所做的预测为条件概率最大的一个类别。这一理论解也叫做贝叶斯分类器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 监督学习算法示例\n",
    "\n",
    "下面举例说明不同的监督学习算法都可以由统计决策理论发展而来。\n",
    "\n",
    "在最邻近算法中，\n",
    "\n",
    "$$\\hat f(x)=Ave(y_i|x_i\\in N_k(x))$$\n",
    "\n",
    "最邻近算法在统计决策理论的基础上加上了两个近似条件来做估计：\n",
    "1. 期望用数据的平均来近似；\n",
    "2. $X=x$这一条件放松成在$X=x$的邻近区域。\n",
    "\n",
    "如果在$X=x$的邻近区域有大量数据，由大数定律可知，$\\hat f(x)\\to E(Y|X=x)$。这时监督学习其实就有了通解。然而在实际应用中，这样的条件并不总是满足。无法满足的主要原因是我们并不总是拥有大量的数据。尤其在高维度数据中，$X=x$的高维度邻近区域所包含的数据往往为零，这就导致了维度灾难。所以最邻近算法仅适用于低维度数据并且数据量较大的情况。\n",
    "\n",
    "另外一种常见的算法是线性回归。线性回归算法首先假设$f(x)\\approx x^T\\beta$，然后利用数据构造残差和（$RSS(\\beta)=\\sum_{i=1}^N(y_i-x_i^T\\beta)^2 $），求使得残差和最小的$\\hat\\beta$：\n",
    "$$\\hat\\beta=(X^TX)^{-1}X^Ty$$\n",
    "\n",
    "这一方法叫做最小二乘法。容易看出，残差和的平均就是对平方损失期望的近似，最小化平方损失的期望就被最小化残差和的平均来代替（而这又等价于最小化残差和）。这一近似的效果也可以从最终结果看到。把$f(x)\\approx x^T\\beta$带入统计决策理论进行推导，求得$\\beta$的理论解为：\n",
    "$$\\beta=[E(X^TX)]^{-1}E(X^Ty)$$\n",
    "\n",
    "\n",
    "可以看到，$\\hat\\beta$就是对$\\beta$的近似，期望用平均来代替。\n",
    "\n",
    "监督学习算法都可以看做是在统计决策理论基础上的某种近似。只能做近似求解的原因在于，我们无法得到期望和条件概率这些概念的理论值，而只能从数据中对这些理论值进行估计。对期望的估计就是平均，对概率的估计就是比率。监督学习的目标就是得到一个对$f(x)$的近似估计$\\hat f(x)$。\n",
    "\n",
    "基于模型的$\\hat f(x)$求解过程为：\n",
    "1. 选定某种类型的监督学习算法，以此给定函数$\\hat f(x)$的基础架构。\n",
    "2. 选定损失函数$L(Y,f(X))$。对于回归问题，通常选用平方损失；对于分类问题，通常采用对数损失（而非0-1损失）。\n",
    "3. 在数据集上对损失进行最小化从而得到$\\hat f(x)$，通常是指得到$\\hat f(x)$中的参数。这一过程叫做经验损失最小化，也叫作拟合数据，或者对数据进行学习。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 方差与偏差：估计的两个误差来源\n",
    "* 模型$\\hat f(x)$复杂度过低（或者模型的假设不正确），偏差大，欠拟合。\n",
    "* 模型$\\hat f(x)$复杂度过高（或者数据量过小），方差大，过拟合（模型过度拟合了数据，实际上拟合的是数据中的噪音）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='lr'></a>3 线性回归方法（Linear Methods for Regression）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 线性回归（Linear Regression）\n",
    "\n",
    "线性回归模型的表达式为：\n",
    "\n",
    "$$f(X)=\\beta_0+\\sum_{j=1}^pX_{j}\\beta_{j}=X\\beta$$\n",
    "\n",
    "使用最小二乘法对$\\beta$进行估计的正规解为：$\\hat\\beta=(X^TX)^{-1}X^Ty$，前提条件是$X$为满秩矩阵。下面利用线性代数证明上述结论。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 证明：当$X$为满秩矩阵时，线性回归的正规解为$\\hat\\beta=(X^TX)^{-1}X^Ty$。\n",
    "\n",
    "使用线性回归模型的残差和可表示为：\n",
    "\n",
    "$$RSS(\\beta)=\\sum_{i=1}^N(y_i-x_i^T\\beta)^2=(y-X\\beta)^T(y-X\\beta)$$\n",
    "\n",
    "要使得残差和最小，需要对其求导，并找到导数为0的条件。\n",
    "\n",
    "首先引入4个线性代数公式：\n",
    "\n",
    "* $trAB=trBA$\n",
    "* $\\bigtriangledown_AtrAB=B^T $\n",
    "* $\\bigtriangledown_{A^T}f(A)=(\\bigtriangledown_Af(A))^T$\n",
    "* $\\bigtriangledown_AtrABA^TC=CAB+C^TAB^T$\n",
    "\n",
    "对残差和求导可得：\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\bigtriangledown_\\beta RSS(\\beta)\n",
    "&=\\bigtriangledown_\\beta (y-X\\beta)^T(y-X\\beta)\\\\\n",
    "&=\\bigtriangledown_\\beta (y^Ty-y^TX\\beta-\\beta^TX^Ty+\\beta^TX^TX\\beta)\\\\\n",
    "&=\\bigtriangledown_\\beta tr(y^Ty-y^TX\\beta-\\beta^TX^Ty+\\beta^TX^TX\\beta)\\\\\n",
    "&=\\bigtriangledown_\\beta [tr(y^Ty)-2tr(y^TX\\beta)+tr(\\beta^TX^TX\\beta)]\\\\\n",
    "&=\\bigtriangledown_\\beta tr(\\beta^TX^TX\\beta)-2\\bigtriangledown_\\beta tr(\\beta y^TX)(上面第一项对\\beta的导数为零)\\\\\n",
    "&=[\\bigtriangledown_{\\beta^T} tr(\\beta^TX^TX\\beta)]^T-2X^Ty\\\\\n",
    "&=[\\beta^TX^TX+\\beta^TX^TX]^T-2X^Ty\\\\\n",
    "&=2X^TX\\beta-2X^Ty\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "当$\\bigtriangledown_\\beta RSS(\\beta)=0$时有：$X^TX\\beta=X^Ty$。\n",
    "\n",
    "下面证明当$X$列向量满秩时，$X^TX$可逆。\n",
    "\n",
    "$X\\in R^{n\\times p}$，当$X$列向量满秩时，$r(X)=p$。则此时$k_1X_1+k_2X_2+k_pX_p=0$没有非零解，即对任意非零向量$a=[k_1,k_2,...,k_p]^T$，都有$Xa\\neq 0$。\n",
    "\n",
    "则内积$(Xa,Xa)=(Xa)^T(Xa)=a^TX^TXa>0$。\n",
    "\n",
    "又$(X^TX)^T=X^TX$，即$X^TX$为对称矩阵，因此$X^TX$为正定矩阵，也就是说$X^TX$可逆，所以就有：\n",
    "\n",
    "$$\\hat\\beta=(X^TX)^{-1}X^Ty$$\n",
    "\n",
    "又$\\frac{\\partial^2RSS}{\\partial\\beta\\partial\\beta^T}=2X^TX>0$，因此当一阶导数为零时求得的极值确实为最小值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有两种情况会导致$X$不是满秩矩阵：\n",
    "* 特征冗余（特征之间线性相关）\n",
    "* 特征过多（特征数多于样本数）\n",
    "\n",
    "若$X$不是满秩矩阵，则无法得到正规解，此时可使用梯度下降法。另外，即使$X$只是接近于不是满秩矩阵，此时正规解对$\\beta$所做的估计也会有较大的方差。遇到这种情况可以尝试删除一些特征，或者使用正则化（regularization）。这两种方法实质为有偏估计。有偏估计与无偏估计相比，偏差会更大，但方差可能会更小。\n",
    "\n",
    "高斯-马尔可夫定律：正规解在所有线性无偏估计中方差最小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 岭回归（Ridge Regression）\n",
    "\n",
    "岭回归在线性回归的基础上，通过最小化含$L2$正则项的残差和进行求解，为有偏估计：\n",
    "\n",
    "$$\\hat\\beta^{ridge}=\\arg\\min_\\beta\\{\\sum_{i=1}^N(y_i-\\beta_0-\\sum_{j=1}^px_{ij}\\beta_j)^2+\\lambda\\sum_{j=1}^p\\beta_j^2\\}$$\n",
    "\n",
    "等价形式为：\n",
    "\n",
    "$$\\hat\\beta^{ridge}=\\arg\\min_\\beta\\sum_{i=1}^N(y_i-\\beta_0-\\sum_{j=1}^px_{ij}\\beta_j)^2, \\sum_{j=1}^p\\beta_j^2\\leqslant t$$\n",
    "\n",
    "注意：正则项中并没有包含$\\beta_0$。\n",
    "\n",
    "岭回归的正规解为：$\\hat\\beta^{ridge}=(X^TX+\\lambda I)^{-1}X^Ty$。通过加入复杂度参数$\\lambda$，即使$X^T X$不是满秩矩阵，$(X^TX+\\lambda I)$也几乎总能变成满秩矩阵，从而使得问题有正规解。下面进行说明。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 证明：复杂度参数使得岭回归的正规解几乎总是存在。\n",
    "\n",
    "岭回归中的数据残差和表达式为：$RSS(\\beta)=(y-X\\beta)^T(y-X\\beta)+\\lambda\\beta^T\\beta$。\n",
    "\n",
    "对数据残差和求导可得：$\\bigtriangledown_\\beta RSS(\\beta)=2X^TX\\beta-2X^Ty+2\\lambda\\beta=0$，即：$(X^TX+\\lambda I)\\beta=X^Ty$。\n",
    "\n",
    "下面讨论$X^TX+\\lambda I$是否可逆。\n",
    "\n",
    "因为$X^TX$为实对称矩阵，所以存在正交矩阵$C$使得：\n",
    "\n",
    "$$C^{-1}X^TXC=\\begin{bmatrix}\n",
    "    \\lambda_1 & & \\\\\n",
    "    & \\lambda_2 & \\\\\n",
    "    & & \\ddots & \\\\\n",
    "    & & & \\lambda_p\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "其中$\\lambda_1,\\lambda_2,...,\\lambda_p$为$X^TX$的特征值，即：$X^TXC_i=\\lambda_iC_i, \\lambda_i\\in\\{\\lambda_1,\\lambda_2,...,\\lambda_p\\}$，其中$C_i$为对应的特征向量。\n",
    "\n",
    "另外又有：$\\lambda IC_i=\\lambda C_i$，所以：$(X^TX+\\lambda I)C_i=(\\lambda_i+\\lambda)C_i$，即：\n",
    "\n",
    "$$C^{-1}(X^TX+\\lambda I)C=\\begin{bmatrix}\n",
    "    \\lambda_1+\\lambda & & \\\\\n",
    "    & \\lambda_2+\\lambda & \\\\\n",
    "    & & \\ddots & \\\\\n",
    "    & & & \\lambda_p+\\lambda\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "即$X^TX+\\lambda I$的特征值为$\\lambda_1+\\lambda,\\lambda_2+\\lambda,...,\\lambda_p+\\lambda$。则有：$|X^TX+\\lambda I|=(\\lambda_1+\\lambda)(\\lambda_2+\\lambda)...(\\lambda_p+\\lambda)$。只要$\\lambda\\neq\\lambda_i, \\lambda_i\\in\\{\\lambda_1,\\lambda_2,...,\\lambda_p\\}$，就有$|X^TX+\\lambda I|\\neq 0$，即$X^TX+\\lambda I$可逆。\n",
    "\n",
    "因此结论就是：当$\\lambda$不等于$X^TX$特征值的相反数时，$X^TX+\\lambda I$可逆，而这种情况几乎总是成立。\n",
    "\n",
    "当$X^TX+\\lambda I$可逆时：\n",
    "\n",
    "$$\\hat\\beta=(X^TX+\\lambda I)^{-1}X^Ty$$\n",
    "\n",
    "特别地，当输入数据$X$为标准正交矩阵时，$\\hat\\beta^{ridge}=\\hat\\beta^{ls}/(1+\\lambda)$，其中$\\hat\\beta^{ls}$为线性回归的正规解。\n",
    "\n",
    "这是因为：当$X$的列向量标准正交时，$X^TX=I$。由岭回归的正规解可得：$\\hat\\beta^{ridge}=\\frac{1}{1+\\lambda}X^Ty$。由线性回归的正规解可得：$\\hat\\beta^{ls}=X^Ty$。\n",
    "\n",
    "因此：$\\hat\\beta^{ridge}=\\hat\\beta^{ls}/(1+\\lambda)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LASSO\n",
    "\n",
    "LASSO在线性回归的基础上，通过最小化含$L1$正则项的残差和进行求解，也为有偏估计：\n",
    "\n",
    "$$\\hat\\beta^{lasso}=\\arg\\min_\\beta⁡\\{\\sum_{i=1}^N(y_i-\\beta_0-\\sum_{j=1}^px_{ij}\\beta_j)^2+\\lambda\\sum_{j=1}^p|\\beta_j|\\}$$\n",
    "\n",
    "等价形式为：\n",
    "\n",
    "$$\\hat\\beta^{lasso}=\\arg\\min_\\beta\\sum_{i=1}^N(y_i-\\beta_0-\\sum_{j=1}^px_{ij}\\beta_j)^2,\\sum_{j=1}^p|\\beta_j|\\leqslant t$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='lc'></a>3 线性分类方法（Linear Methods for Classification）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 分类问题综述\n",
    "\n",
    "分类问题可以用以下两种不同的方式进行求解：\n",
    "* 第一种方法是首先对条件概率$Pr⁡(G_k|X=x)$进行估计，然后利用得到的条件概率估计以及其他的辅助信息来做分类决策。这一方法把概率估计和分类决策这两个步骤进行了区分。任何可以对条件概率$Pr⁡(G_k|X=x)$进行估计的模型都属于这一类分类方法，包括所有的回归方法，线性判别分析，决策树，集成学习以及神经网络等等。\n",
    "* 另一种方法直接对$\\hat G(x)$进行估计，不再分成概率估计和分类决策两个步骤。这样的模型直接对决策边界进行建模，并且不会输出对条件概率的估计。这样的分类模型有感知机和支持向量机。\n",
    "\n",
    "对于多类别分类问题，常用方法是一对多分类法：对于任意类别$k$，对这一类别与其他所有类别进行区分，并对所有的$K$个类别都进行同样的区分动作。对每个类别进行建模分类等同于解决一个二元分类问题（属于当前类别和不属于当前类别），因此多类别问题可以转化为若干个二元分类问题进行解决。下面对二元分类问题进行讨论。\n",
    "\n",
    "分类问题通常使用对数损失。在二元分类问题中，所有数据的对数损失之和表示为（$y=0$或$y=1$）：\n",
    "\n",
    "$$J(\\beta)=-\\sum_{i=1}^N\\{y_i\\ln p(y_i=1|x_i;\\beta)+(1-y_i)\\ln⁡(1-p(y_i=1|x_i;\\beta))\\}$$\n",
    "\n",
    "事实上，通过最小化对数损失对$\\beta$进行估计的方法等同于最大似然估计。二元分类问题的似然函数为：\n",
    "\n",
    "$$L(\\beta)=\\prod_{i=1}^{N}p_{y_i}(x_i;\\beta)=\\prod_{i=1}^{N}p(y_i=1|x_i;\\beta)^{y_i}(1-p(y_i=1|x_i;\\beta))^{1-y_i}$$\n",
    "\n",
    "对上式取对数，得到对数似然函数：\n",
    "$$l(\\beta)=\\ln L(\\beta)=\\sum_{i=1}^N\\{y_i\\ln p(y_i=1|x_i;\\beta)+(1-y_i)\\ln⁡(1-p(y_i=1|x_i;\\beta))\\}$$\n",
    "\n",
    "因此，最大化对数似然函数等价于最小化对数损失。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 线性判别分析（Linear Discriminant Analysis，LDA）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "应用贝叶斯公式：\n",
    "\n",
    "$$Pr⁡(G=k│X=x)=\\frac{f_k(x)\\pi_k}{\\sum_{l=1}^Kf_l(x)\\pi_l}$$\n",
    "\n",
    "其中$f_k(x)$为类别$k$的概率密度，$\\pi_k$为类别$k$的先验概率。\n",
    "\n",
    "线性判别分析模型的两个假设：\n",
    "* 假设类别$k$的概率密度为多元高斯分布：\n",
    "$f_k(x)=\\frac{1}{(2\\pi)^{p/2}|\\Sigma_k|^{1/2}}e^{-1/2(x-\\mu_k)^T \\Sigma_k^{-1}(x-\\mu_k)}$。\n",
    "* 不同类别的协方差矩阵相同：$\\Sigma_k=\\Sigma$ $\\forall k$。\n",
    "\n",
    "对模型中的参数进行最大似然估计：\n",
    "\n",
    "* $\\hat\\pi_k=N_k/N$\n",
    "* $\\hat\\mu_k=\\Sigma_{g_i=k}x_i/N_k$\n",
    "* $\\hat\\Sigma=\\Sigma_{k=1}^K\\Sigma_{g_i=k}(x_i-\\hat\\mu_k)(x_i-\\hat\\mu_k)^T/(N-K)$\n",
    "\n",
    "二次判别分析（Quadratic Discriminant Analysis，QDA）：并不假设不同类别的$\\Sigma_k$相同。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 逻辑回归（Logistic Regression）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于二元分类问题，逻辑回归模型的表达式为：\n",
    "\n",
    "$$p(y=1|X=x;\\theta)=\\frac{1}{1+e^{-\\beta^Tx}}$$\n",
    "\n",
    "数据的对数损失之和化简为：\n",
    "\n",
    "$$J(\\beta)=-\\sum_{i=1}^N\\{y_i\\beta^Tx_i-\\ln(1+e^{\\beta^Tx_i})\\}$$\n",
    "\n",
    "最小化对数损失的方法有梯度下降法和牛顿迭代法。\n",
    "\n",
    "与线性判别分析相比，逻辑回归更具有一般性。逻辑回归的假设更少，模型中$X$的边缘分布可以是任意密度函数$Pr(X)$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='assessment'></a>7 模型评估和选择（Model Assessment and Selection）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型误差\n",
    "\n",
    "* 预测误差（测试误差或者泛化误差）：$Err_T=E[L(Y,\\hat f(X))|T]$，$T$为训练集。\n",
    "\n",
    "* 预测误差期望：$Err=E[L(Y,\\hat f(X))]=E[Err_T]$。\n",
    "\n",
    "为了进行模型选择和评估，我们的目的是对预测误差进行估计，但是对于统计分析来说估计预测误差期望更合适也往往更容易实现。\n",
    "\n",
    "#### 模型选择\n",
    "\n",
    "对不同模型的误差进行估计，从而选择最好的一个：\n",
    "\n",
    "* 数据量较大时，在验证集上估计预测误差；\n",
    "* 数据量较小时，使用交叉验证（cross validation）法或自助法（bootstrap）来估计预测误差期望。\n",
    "\n",
    "#### 模型评估\n",
    "\n",
    "在测试集（新数据）上估计最终模型的预测误差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='inference'></a>8 模型推断和平均（Model Inference and Averaging）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bagging（Bootstrap Aggregation）\n",
    "\n",
    "Bagging属于集成学习，对自助法估计结果进行平均：$\\hat f_{bag}(x)=\\frac{1}{B}\\sum_{b=1}^B\\hat f^{\\ast b}(x)$\n",
    "\n",
    "使用Bagging的注意点：\n",
    "\n",
    "* 原始模型$\\hat f(x)$需要非线性。原始模型线性时，Baggging的估计期望与原始估计的期望相同，无法改善估计结果。\n",
    "* 损失函数为平方损失时Bagging很有用，因为它可以降低方差同时保持偏差不变。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='tree'></a>9 可加模型，树模型以及相关方法（Additive Models, Trees, and Related Methods）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CART（分类与回归树）\n",
    "\n",
    "树模型采取“分而治之”策略，把特征空间划分成一个个长方形的区域，然后对每一个区域内的数据拟合一个简单的模型（例如常数）。\n",
    "\n",
    "另外，这里的树模型指二叉树模型。\n",
    "\n",
    "回归树\n",
    "\n",
    "回归树模型的表达式为：\n",
    "\n",
    "$$f(x)=\\sum_{m=1}^Mc_m I(x\\in R_m)$$\n",
    "$$\\hat c_m=ave(y_i|x_i\\in R_m)$$\n",
    "\n",
    "回归树模型的求解过程是：\n",
    "\n",
    "1. 确定最佳划分变量$j$和最佳划分点$s$：\n",
    "\n",
    "    $$\\min_{j,s}⁡[\\min_{c_1}\\sum_{x_i\\in R_1(j,s)}(y_i-c_1)^2 +\\min_{c_2}\\sum_{x_i\\in R_2(j,s)}(y_i-c_2)^2]$$\n",
    "\n",
    "    其中：\n",
    "\n",
    "    $$R_1(j,s)=\\{X|X_j\\leqslant s\\}, R_2(j,s)=\\{X|X_j>s\\}$$\n",
    "    $$\\hat c_1=ave(y_i│x_i\\in R_1(j,s)), \\hat c_2=ave(y_i│x_i\\in R_2(j,s))$$\n",
    "\n",
    "2. 使用变量$j$和划分点$s$进行划分后，在每个区域内重复上述过程，直到回归树的叶节点大小都小于或等于设定值，此时形成的树记做$T_0$。\n",
    "\n",
    "3. 通过最小化成本复杂度对$T_0$进行剪枝，降低树的复杂度，防止过拟合。\n",
    "\n",
    "成本复杂度：$C_\\alpha(T)=\\sum_{m=1}^{|T|}N_mQ_m(T)+\\alpha|T|$，其中：\n",
    "\n",
    "* $N_m=\\#\\{x_i\\in R_m\\}$\n",
    "* $\\hat c_m=\\frac{1}{N_m}\\sum_{x_i\\in R_m}y_i$\n",
    "* $Q_m(T)=\\frac{1}{N_m}\\sum_{x_i\\in R_m}(y_i-\\hat c_m)^2$\n",
    "\n",
    "分类树\n",
    "\n",
    "\n",
    "\n",
    "分类树与回归树相比，唯一不同的是划分节点和进行剪枝所使用的节点杂度$Q_m(T)$的测量方法：\n",
    "\n",
    "* 误分率：$\\frac{1}{N_m}\\sum_{i\\in R_m}I(y_i\\neq k(m))=1-\\hat p_{mk(m)}$，其中$k(m)=\\arg\\max_k\\hat p_{mk}$。\n",
    "* 基尼指数：$\\sum_{k\\neq k'}\\hat p_{mk}\\hat p_{mk'}=\\sum_{k=1}^K\\hat p_{mk}(1-\\hat p_{mk})$\n",
    "* 交叉熵（cross-entropy，也叫作deviance）：$-\\sum_{k=1}^K\\hat p_{mk}\\ln \\hat p_{mk}$\n",
    "\n",
    "其中$\\hat p_{mk}$为节点$m$中类别$k$所占的比率为：$\\hat p_{mk}=\\frac{1}{N_m}\\sum_{x_i\\in R_m}I(y_i=k)$\n",
    "\n",
    "生成分类树可以使用基尼指数或交叉熵。进行剪枝可以使用任意一种测量方法，但通常使用误分率。\n",
    "\n",
    "树模型的一大问题是其模型复杂度较高导致方差较大，容易过拟合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='boosting'></a>10 Boosting与可加树模型（Boosting and Additive Trees）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting算法的直观理解是不断调整样本数据的权重然后进行训练，依次产生一系列弱分类器，最后集成这些弱分类器形成强分类器。讨论Boosting方法时，对二元分类问题的输出编码为：$Y\\in\\{-1,1\\}$。\n",
    "\n",
    "#### AdaBoost算法\n",
    "\n",
    "1. 初始化数据权重$w_i=1/N$，$i=1,2,…,N$。\n",
    "2. 从$m=1$到$M$：\n",
    "\n",
    "\t(a) 对权重为$w_i$的训练数据进行拟合得到分类器$G_m(x)$。\n",
    "    \n",
    "\t(b) 计算：\n",
    "\n",
    "    $$err_m=\\frac{\\sum_{i=1}^Nw_iI(y_i\\neq G_m(x_i))}{\\sum_{i=1}^Nw_i}$$\n",
    "\n",
    "\t(c) 计算$\\alpha_m=\\ln⁡((1-err_m)/err_m)$。\n",
    "    \n",
    "\t(d) 更新权重：$w_i\\leftarrow w_i\\cdot exp⁡[\\alpha_m\\cdot I(y_i\\neq G_m(x_i))], i=1,2,…,N$。\n",
    "3. 输出：$G(x)=sign[\\sum_{m=1}^M\\alpha_mG_m(x)]$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdaBoost算法等价于使用指数损失来求解前向逐步叠加模型（Foward Stagewise Additive Modeling）。\n",
    "\n",
    "证明：\n",
    "\n",
    "在第$m$步，前向逐步叠加模型对$f(x)$的更新公式为：$f_m(x)=f_{m-1}(x)+\\beta G(x)$，模型参数为$\\beta$和$G(x)$。\n",
    "\n",
    "指数损失：$L(Y,f(X))=exp⁡(-Yf(X))$，其中$Y\\in\\{-1,1\\}$。\n",
    "\n",
    "利用统计决策理论，通过最小化经验指数损失之和来求解前向逐步叠加模型中的参数：\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "(\\beta_m,G_m)\n",
    "&=\\arg\\min_{\\beta,G}\\sum_{i=1}^{N}exp[-y_i(f_{m-1}(x_i)+\\beta G(x_i))]\\\\\n",
    "&=\\arg\\min_{\\beta,G}\\sum_{i=1}^{N}w_i^{(m)}exp[-\\beta y_iG(x_i)]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "其中$w_i^{(m)}=exp[-y_if_{m-1}(x_i)]$\n",
    "\n",
    "进一步转化得：\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "(\\beta_m,G_m)\n",
    "&=\\arg\\min_{\\beta,G}\\{\\sum_{i=1}^{N}w_i^{(m)}e^{-\\beta}I(y_i=G(x_i))+\\sum_{i=1}^{N}w_i^{(m)}e^{\\beta}I(y_i\\neq G(x_i))\\}\\\\\n",
    "&=\\arg\\min_{\\beta,G}\\{e^{-\\beta}\\sum_{i=1}^{N}w_i^{(m)}I(y_i=G(x_i))+e^{\\beta}\\sum_{i=1}^{N}w_i^{(m)}I(y_i\\neq G(x_i))\\}\\\\\n",
    "&=\\arg\\min_{\\beta,G}\\{e^{-\\beta}\\sum_{i=1}^{N}w_i^{(m)}+(e^{\\beta}-e^{-\\beta})\\sum_{i=1}^{N}w_i^{(m)}I(y_i\\neq G(x_i))\\}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "所以，\n",
    "\n",
    "$$G_m=\\arg\\min_G\\sum_{i=1}^{N}w_i^{(m)}I(y_i\\neq G(x_i))$$\n",
    "\n",
    "这对应Adaboost算法中的2a。\n",
    "\n",
    "下面通过对上述损失进行求导来求解另一个参数$\\beta_m$。\n",
    "\n",
    "$$L(\\beta)=\\sum_{i=1}^Nw_i^{(m)}\\cdot[e^{-\\beta}+\\frac{(e^\\beta-e^{-\\beta})\\sum_{i=1}^Nw_i^{(m)}I(y_i\\neq G(x_i))}{\\sum_{i=1}^Nw_i^{(m)}}]$$\n",
    "\n",
    "设$err_m=\\frac{\\sum_{i=1}^Nw_i^{(m)}I(y_i\\neq G(x_i))}{\\sum_{i=1}^Nw_i^{(m)}}$（对应Adaboost算法中的2b），则$L(\\beta)$可化简为：\n",
    "\n",
    "$$L(\\beta)=\\sum_{i=1}^Nw_i^{(m)}\\cdot[e^{-\\beta}+err_m(e^\\beta-e^{-\\beta})]$$\n",
    "\n",
    "对$L(\\beta)$进行求导得：\n",
    "\n",
    "$$\\frac{\\partial L(\\beta)}{\\partial \\beta}=\\sum_{i=1}^Nw_i^{(m)}\\cdot[-e^{-\\beta}+err_m(e^\\beta+e^{-\\beta})]=0$$\n",
    "\n",
    "得：\n",
    "\n",
    "$$\\beta_m=\\frac{1}{2}\\ln\\frac{1-err_m}{err_m}$$\n",
    "\n",
    "此时有：\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "w_i^{m+1}\n",
    "&=e^{-y_if_m(x_i)}\\\\\n",
    "&=e^{-y_i(f_{m-1}(x)+\\beta_m G_m(x))}\\\\\n",
    "&=w_i^{m}e^{-y_i\\beta_m G_m(x)}\\\\\n",
    "&=w_i^{m}e^{[2I(y_i\\neq G_m(x))-1]\\beta_m}\\\\\n",
    "&=w_i^{m}e^{2\\beta_mI(y_i\\neq G_m(x))}e^{-\\beta_m}\\\\\n",
    "&=w_i^{m}e^{\\alpha_mI(y_i\\neq G_m(x))}e^{-\\beta_m}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "其中$\\alpha_m=2\\beta_m=\\ln\\frac{1-err_m}{err_m}$，这一结果对应Adaboost算法中的2c和2d（$e^{-\\beta_m}$不会对数据间的相对权重产生改变因此不改变结果）。证毕。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 指数损失\n",
    "\n",
    "使用指数损失，利用统计决策理论得到的最优解为：\n",
    "\n",
    "$$f^{\\ast}(x)=\\frac{1}{2}\\ln \\frac{Pr⁡(Y=1|x)}{Pr⁡(Y=-1|x)}$$\n",
    "\n",
    "下面进行证明。\n",
    "\n",
    "令$Z=e^{-Yf(X)}$，其中$X$和$Y$均为随机变量。此问题的目标是求得函数$f(X)$，使得$E(Z)$最小。与平方损失的推导相同，此问题需要求得使$E(Z|X=x)$最小的$f(x)$即可。\n",
    "\n",
    "$E(Z|X=x)=E(e^{-Yf(X)}|X=x)=Pr(Y=1|X=x)e^{-f(x)}+Pr(Y=-1|X=x)e^{f(x)}$\n",
    "\n",
    "对上式求导，并令结果为0得：\n",
    "\n",
    "$-Pr(Y=1|X=x)e^{-f(x)}+Pr(Y=-1|X=x)e^{f(x)}=0$\n",
    "\n",
    "解得$f^\\ast (x)=\\frac{1}{2}\\ln \\frac{Pr⁡(Y=1|x)}{Pr⁡(Y=-1|x)}$。\n",
    "\n",
    "#### deviance（也叫作交叉熵）\n",
    "\n",
    "与指数损失有着相同最优解的另一种损失函数为deviance：\n",
    "\n",
    "$L(Y,f(X))=\\ln⁡(1+e^{-2Yf(X)})$，其中$Y\\in\\{-1,1\\}$。\n",
    "\n",
    "容易看出，这一损失与指数损失（$L(Y,f(X))=e^{-Yf(X)}$）有着相同的单调变化规律，因此两者的期望会在同一点$f^\\ast(x)$达到最小值。\n",
    "\n",
    "另外，deviance其实就是逻辑回归中使用的对数损失：\n",
    "\n",
    "$L(Y,f^\\ast(x))=\\ln⁡(1+e^{-2Yf^\\ast(x)})=-[Y'\\ln Pr(Y'=1|X=x)+(1-Y')\\ln(1-Pr(Y'=1|X=x))]$，其中$Y\\in\\{-1,1\\}$，$Y'\\in\\{0,1\\}$。\n",
    "\n",
    "证明：\n",
    "\n",
    "$Y$与$Y'$之间的关系为：$Y'=(Y+1)/2$，\n",
    "\n",
    "最优解为：$f^\\ast(x)=\\frac{1}{2}\\frac{Pr(Y=1|X=x)}{Pr(Y=-1|X=x)}=\\frac{1}{2}\\frac{Pr(Y'=1|X=x)}{Pr(Y'=0|X=x)}$\n",
    "\n",
    "因此：\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "L(Y,f^\\ast(x))\n",
    "&=\\ln⁡(1+e^{-2Yf^\\ast(x)})\\\\\n",
    "&=\\ln(1+e^{-Y\\ln\\frac{Pr(Y=1|X=x)}{Pr(Y=-1|X=x)}})\\\\\n",
    "&=\\ln(1+e^{-(2Y'-1)\\ln\\frac{Pr(Y'=1|X=x)}{Pr(Y'=0|X=x)}})\\\\\n",
    "&=\\ln(1+[\\frac{Pr(Y'=0|X=x)}{Pr(Y'=1|X=x)}]^{(2Y'-1)})\\\\\n",
    "&=\\ln(1+\\frac{Pr(Y'=0|X=x)}{Pr(Y'=1|X=x)}\\cdot I(Y'=1)+\\frac{Pr(Y'=1|X=x)}{Pr(Y'=0|X=x)}\\cdot I(Y'=0))\\\\\n",
    "&=\\ln(I(Y'=1)+I(Y'=0)+\\frac{Pr(Y'=0|X=x)}{Pr(Y'=1|X=x)}\\cdot I(Y'=1)+\\frac{Pr(Y'=1|X=x)}{Pr(Y'=0|X=x)}\\cdot I(Y'=0))\\\\\n",
    "&=\\ln(\\frac{I(Y'=1)}{Pr(Y'=1|X=x)}+\\frac{I(Y'=0)}{Pr(Y'=0|X=x)})\\\\\n",
    "&=-I(Y'=1)\\ln Pr(Y'=1|X=x)-I(Y'=0)\\ln Pr(Y'=0|X=x)\\\\\n",
    "&=-[Y'\\ln Pr(Y'=1|X=x)+(1-Y')\\ln Pr(Y'=0|X=x)]\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 损失函数的比较\n",
    "\n",
    "#### 分类问题\n",
    "\n",
    "已经遇到的或者可能使用的损失函数有（$y\\in\\{-1,1\\}$）：\n",
    "\n",
    "* 0-1损失（误分类损失）：$L(y,f(x))=I(yf(x)<0)$\n",
    "* deviance（对数损失）：$L(y,f(x))=\\ln⁡(1+e^{-2yf(x)})$\n",
    "* 指数损失：$L(y,f(x))=e^{-yf(x)}$\n",
    "* Hinge损失（支持向量机使用这一损失）：$L(y,f(x))=(1-yf(x))_+$\n",
    "\n",
    "其中$yf(x)$叫做“间隔”，其与回归问题中残差$y-f(x)$的作用类似。正间隔表示分类正确，负间隔表示分类错误，因此负间隔的损失函数值需要比正间隔大，且损失函数应随着间隔的增加而单调递减。因此，平方损失不能用于分类问题，其并不是一个关于间隔单调递减的函数。\n",
    "\n",
    "分类问题不同损失函数之间的关系是：\n",
    "\n",
    "* 使用指数损失和deviance可以得到相同的理论最优解，两者都可以看做是对0-1损失的单调连续近似。这两者的区别在于：对于较大的负间隔，指数损失比deviance要大很多。这意味着，当样本数据中有异常值（例如类别标记错误）时，指数损失会格外在意这些异常值，而deviance相对来说不容易受到异常值的干扰。这就导致了使用指数损失的算法不如使用deviance的算法性能稳定。\n",
    "\n",
    "#### 回归问题\n",
    "\n",
    "已经遇到的或者可能使用的损失函数有：\n",
    "\n",
    "* 平方损失：$L(y,f(x))=(y-f(x))^2$\n",
    "* 绝对损失：$L(y,f(x))=|y-f(x)|$\n",
    "\n",
    "平方损失和绝对损失的关系与指数损失和deviance的关系类似。使用平方损失的理论最优解为$f(x)=E(Y|x)$，使用绝对损失的理论最优解为$f(x)=median(Y|x)$。当样本数据中有异常值（例如测量错误）时，平方损失会格外在意这些异常值，而绝对损失相对来说不容易受到异常值的干扰。\n",
    "\n",
    "从模型鲁棒性的角度上说，平方损失和指数损失都不是最好的选择。但对于boosting方法来说，这两种损失的优点在于可以得到简明的算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 梯度Boosting\n",
    "\n",
    "Boosting树是指以树模型为基模型的前向逐步叠加模型。若使用指数损失，那么Boosting树的求解步骤等同于Adaboost算法，对每一棵基树的求解也与一般的树模型一致。但若为了模型的鲁棒性使用deviance作为损失函数，这时就没有对应的简明boosting算法而需要使用数值方法：梯度Boosting。梯度Boosting可以理解成梯度下降法与Boosting树相结合的算法。\n",
    "\n",
    "一般来说，监督学习都是找到使得经验损失最小的$f$，这相当于把损失函数对$f$求导，取导数为0时的$f$。若无法得到分析解，还可以采用数值方法逼近分析解，其中一种数值方法就是梯度下降法：\n",
    "\n",
    "$$\\textbf{f}_m=\\textbf{f}_{m-1}-\\rho_m \\textbf{g}_m$$\n",
    "\n",
    "其中$\\textbf{g}_m$为梯度，$g_{im}=[\\frac{\\partial L(y_i,f(x_i))}{\\partial f(x_i)}]_{f(x_i)=f_{m-1}(x_i)}$。对于任何可微分的损失函数来说这一梯度都很好计算。\n",
    "\n",
    "另外，Boosting树可表示为：\n",
    "\n",
    "$$f_m=f_{m-1}+T(x;\\Theta_m)$$\n",
    "\n",
    "梯度Boosting就是把梯度下降法与Boosting树相结合，通过最小二乘法对负梯度值进行拟合得到回归树$T$：\n",
    "\n",
    "$$\\tilde\\Theta_m=\\arg\\min_{\\Theta_m}\\sum_{i=1}^N(-g_{im}-T(x_i;\\Theta_m))^2$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boosting树的实际经验总结\n",
    "\n",
    "可调参数：\n",
    "\n",
    "* $M$，boosting迭代数（树的个数）；\n",
    "* $J$，单棵树叶节点的最多个数，限定所有树都相同。经验表明可取$4\\leq J\\leq 8$；\n",
    "* $\\gamma$，收缩参数，降低每棵树在当前估计中所占的比重，控制boosting的学习速率，缓解过拟合。\n",
    "\n",
    "常用方法是设定较小的收缩参数($\\gamma<0.1$)和较大的迭代次数$M$，通过提前停止的方式控制拟合度。\n",
    "\n",
    "#### 相对重要性（Relative Importance）\n",
    "\n",
    "度量输入变量各个特征与输出变量的关联程度。\n",
    "\n",
    "变量$X_l$的平方相对重要性：当变量$X_l$被选作划分变量时，其在所有树和所有类别上对平方损失降低的平均值。相对重要性是平方相对重要性的平方根。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='nn'></a>11 神经网络（Neural Networks）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络不是本笔记的重点，不做过多记录。\n",
    "\n",
    "从数学上看，神经网络只不过是一种非线性统计模型。神经网络对于高信噪比、只需要预测不需要模型提供解释的问题特别有效，而对于需要描述数据产生机理或者解释输入特征作用的问题就没那么有用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='rf'></a>15 随机森林（Random Forest）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随机森林：Bagging（基模型为树模型）+ 随机选择输入变量。随机森林可以进一步降低Bagging的方差。\n",
    "\n",
    "包外（Out of Bag，OOB）误差：随机森林模型的一大特色是可以使用包外误差来估计模型的预测性能。包外数据是指使用自助法（bootstrap）选择数据时一次也没有选择到的数据所组成的数据集。包外误差与交叉验证误差结果相似。\n",
    "\n",
    "当与输出相关联的输入变量个数较少、而随机选择输入变量的个数又太少时，随机森林的性能会很差，相反随机森林的性能会很稳定。\n",
    "\n",
    "随机森林不会导致过拟合。经验表明，随机森林中使用完全长成的树不会导致不良后果，而这样做的好处是可以减少一个需要调整的超参数。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
