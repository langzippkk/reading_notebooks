{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读书笔记：统计学习基础（第2版）（英文）\n",
    "### The Elements of Statistical Learning: Data Mining, Inference, and Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 原书链接\n",
    "\n",
    "* [官方免费电子版下载](https://web.stanford.edu/~hastie/ElemStatLearn/)\n",
    "* [纸质书购买](https://www.amazon.cn/dp/B00PRH2BXA/ref=sr_1_2?ie=UTF8&qid=1547965512&sr=8-2&keywords=%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0)\n",
    "\n",
    "### 主要内容\n",
    "#### 第一章：（略）\n",
    "#### 第二章：[监督学习综述（Overview of Supervised Learning）](#overview)\n",
    "#### 第三章：[线性回归方法（Linear Methods for Regression）](#lr)\n",
    "#### 第四章：[线性分类方法（Linear Methods for Classification）](#lc)\n",
    "#### 第五章：（略）\n",
    "#### 第六章：（略）\n",
    "#### 第七章：[模型评估和选择（Model Assessment and Selection）](#assessment)\n",
    "#### 第八章：[模型推断和平均（Model Inference and Averaging）](#inference)\n",
    "#### 第九章：[可加模型，树模型以及相关方法（Additive Models, Trees, and Related Methods）](#tree)\n",
    "#### 第十章：[增强模型和可加树模型（Boosting and Additive Trees）](#boosting)\n",
    "#### 第十一章：[神经网络（Neural Networks）](#nn)\n",
    "\n",
    "\n",
    "### 笔记功能\n",
    "本读书笔记并不是原书的摘要版。如果想要对机器学习做系统地了解，本人建议直接阅读原书。本读书笔记的作用在于：\n",
    "1. 推导书中的一些数学结论；\n",
    "2. 梳理本人感兴趣的主要内容，以做记录。\n",
    "\n",
    "### 更新记录\n",
    "01/22/2019 - 完成第二章：监督学习综述"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='overview'></a>2 监督学习综述（Overview of Supervised Learning）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 统计决策理论（Statistical Decision Theory）\n",
    "监督学习的数学理论是统计决策理论，对于统计决策理论的介绍也是本书区别于其他机器学习教材和课程的特色之一。只有理解了统计决策理论，才能把各种监督学习算法联系起来，并以此为基础创造出新的变化。\n",
    "\n",
    "统计决策理论可以描述为：\n",
    "\n",
    "若输入变量$X$和输出变量$Y$均为随机变量。求函数$f(X)$，对指定的$X=x$，$f(x)$可以对$Y$做出预测。\n",
    "\n",
    "求得$f(X)$的过程为：\n",
    "\n",
    "1. 定义损失函数（loss function）$L(Y,f(X))$。损失函数描述预测值$f(X)$与$Y$之间的差异。\n",
    "2. 最小化损失函数的期望$E(L(Y,f(X)))$，得到对应的$f(X)$。\n",
    "\n",
    "不同的损失函数所对应的$f(x)$有所不同。若定义损失函数为方差损失：\n",
    "\n",
    "$$L(Y,f(X))=(Y-f(X))^2$$\n",
    "\n",
    "此时求得$f(x)$为：\n",
    "\n",
    "$$f(x)=E(Y│X=x)$$\n",
    "\n",
    "即对任意$X=x$，$f(x)$所做的预测为当$X=x$时$Y$的条件期望。\n",
    "\n",
    "下面对上述结论进行证明。\n",
    "\n",
    "假设$Z=(Y-f(X))^2$，其中$X$和$Y$均为随机变量。此问题的目标是求得函数$f(X)$，使得$E(Z)$最小。\n",
    "\n",
    "由连续型随机变量的数学期望定义可得：\n",
    "$$E(Z)=\\int(y-f(x))^2g(x,y)dxdy$$\n",
    "其中$g(x,y)$为$(X,Y)$的联合概率密度。\n",
    "\n",
    "又连续型随机变量的条件分布为：$g_{Y|X}=g(x,y)/g_x(x)$，所以：\n",
    "$$E(Z)=\\int(y-f(x))^2g_{Y|X}g_X(x)dxdy=\\int_xg_X(x)dx\\int_y(y-f(x))^2g_{Y|X}dy$$\n",
    "\n",
    "所以要使得$E(Z)$最小，就需要$\\int_y(y-f(x))^2g_{Y|X}dy$最小即可。\n",
    "\n",
    "下面利用随机变量的方差的性质：\n",
    "\n",
    "对一切实数$C$，$DX=E(X-EX)^2\\leqslant E(X-C)^2$。\n",
    "\n",
    "所以：\n",
    "$$\\int_y(y-f(x))^2g_{Y|X}dy=E[(Y-f(X))^2|X=x]\\geqslant E[(Y-E(Y|X=x)^2|X=x]$$\n",
    "\n",
    "即当$f(x)=E(Y|X=x)$时，$E(Z)$取得最小值，得证。\n",
    "\n",
    "上面的讨论都是基于回归问题。分类问题与回归问题类似，下面进行说明。\n",
    "\n",
    "\n",
    "对于任意损失函数：$L(G,\\hat G(X))$，由统计决策理论求得的$\\hat G(x)$（类比于分类问题中的$f(x)$）为：\n",
    "$$\\hat G(x)=argmin_{g\\in G}\\sum_{k=1}^KL(G_k,g)Pr⁡(G_k|X=x)$$\n",
    "\n",
    "若定义0-1损失：$L(G,\\hat G(X))=1\\cdot I(G\\neq \\hat G(X))$，那么所求得的$\\hat G(x)$为：\n",
    "\n",
    "$$\\hat G(x)=argmin_{g\\in G}[1-Pr⁡(g│X=x)]$$\n",
    "\n",
    "即对任意$X=x$，$\\hat G(X)$所做的预测为条件概率最大的一个类别。这一理论解也叫做贝叶斯分类器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 监督学习算法\n",
    "下面举例说明不同的监督学习算法都可以由统计决策理论发展而来。\n",
    "\n",
    "在最邻近算法中，\n",
    "\n",
    "$$\\hat f(x)=Ave(y_i|x_i\\in N_k(x))$$\n",
    "\n",
    "最邻近算法在统计决策理论的基础上加上了两个近似条件来做估计：\n",
    "1. 期望用数据的平均来近似；\n",
    "2. $X=x$这一条件放松成在$X=x$的邻近区域。\n",
    "\n",
    "如果在$X=x$的邻近区域有大量数据，由大数定律可知，$\\hat f(x)\\to E(Y|X=x)$。这时监督学习其实就有了通解。然而在实际应用中，这样的条件并不总是满足。无法满足的主要原因是我们并不总是拥有大量的数据。尤其在高维度数据中，$X=x$的高维度邻近区域所包含的数据往往为零，这就导致了维度灾难。所以最邻近算法仅适用于低维度数据并且数据量较大的情况。\n",
    "\n",
    "另外一种常见的算法是线性回归。线性回归算法首先假设$f(x)\\approx x^T\\beta$，然后采用最小二乘法，利用数据构造残差和（$RSS(\\beta)=\\sum_{i=1}^N(y_i-x_i^T\\beta)^2 $），求使得残差和最小的$\\hat\\beta$：\n",
    "$$\\hat\\beta=(X^TX)^{-1}X^Ty$$\n",
    "\n",
    "容易看出，残差和的平均就是对方差损失期望的近似，最小化方差损失的期望就被最小化残差和的平均来代替（而这又等价于最小化残差和）。这一近似的效果也可以从最终结果看到。把$f(x)\\approx x^T\\beta$带入统计决策理论进行推导，求得$\\beta$的理论解为：\n",
    "$$\\beta=[E(X^TX)]^{-1}E(X^Ty)$$\n",
    "\n",
    "\n",
    "可以看到，$\\hat\\beta$就是对$\\beta$的近似，期望用平均来代替。\n",
    "\n",
    "监督学习算法都可以看做是在统计决策理论基础上的某种近似。只能做近似求解的原因在于，我们无法得到期望和条件概率这些概念的理论值，而只能从数据中对这些理论值进行估计。对期望的估计就是平均，对概率的估计就是比率。监督学习的目标就是得到一个对$f(x)$的近似估计$\\hat f(x)$。\n",
    "\n",
    "基于模型的$\\hat f(x)$求解过程为：\n",
    "1. 选定某种类型的监督学习算法，以此给定函数$\\hat f(x)$的基础架构。\n",
    "2. 选定损失函数$L(Y,f(X))$。对于回归问题，通常选用方差损失；对于分类问题，通常采用对数损失（而非0-1损失）。\n",
    "3. 在数据集上对损失（或者平均损失）进行最小化从而得到$\\hat f(x)$，通常是指得到$\\hat f(x)$中的参数。这一过程叫做经验损失最小化，也叫作拟合数据，或者对数据进行学习。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 方差与偏差：估计的误差来源\n",
    "* 模型$\\hat f(x)$复杂度过低（或者模型的假设不正确），偏差大，欠拟合。\n",
    "* 模型$\\hat f(x)$复杂度过高（或者数据量过小），方差大，过拟合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='lr'></a>3 线性回归方法（Linear Methods for Regression）"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
